# -*- coding: utf-8 -*-
"""News (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BKbUUTqVHwBYGsncXa3WcvPvHnlPZHn2
"""

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service as ChromeService
from bs4 import BeautifulSoup
import time

def is_social_media_or_wikipedia(url):
    return any(domain in url for domain in ['twitter', 'facebook', 'instagram', 'linkedin', 'pinterest', 'wikipedia'])

def get_google_search_links(keyword, num_results):
    url = f"https://www.google.com/search?q={keyword}"

    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)

    try:
        driver.get(url)

        # Scroll down the page to load more results
        for _ in range(num_results // 10):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        # Get the page source containing search results
        page_source = driver.page_source

        # Extract URL links from the search results
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(page_source, 'html.parser')
        links = []
        for result in soup.find_all('div', class_='tF2Cxc'):
            link = result.find('a')
            if link:
                link_url = link['href']
                if not is_social_media_or_wikipedia(link_url):
                    links.append(link_url)

        return links[:num_results]

    except Exception as e:
        print(f"Error: {e}")
        return []

    finally:
        driver.quit()
article_links = []
def main():
    keywords = input("Enter the keywords to search (comma-separated): ").split(',')
    num_results = int(input("Enter the number of links to scrape: "))

    for keyword in keywords:

        article_link = get_google_search_links(keyword, num_results)
        article_links.append(article_link)



    print(f"URL links from Google search results for '{keywords}':")
    for idx, link in enumerate(article_links, 1):
        print(f"{idx}. {link}")

if __name__ == "__main__":
    main()

!pip install wordcloud

article_links
flattened_list = [element for sublist in article_links for element in sublist]
print(flattened_list)

!pip install transformers





!pip install TensorFlow

sentiment_pipeline = pipeline("sentiment-analysis")

from newspaper import Article
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from newspaper import Config
from wordcloud import WordCloud
from collections import Counter
import nltk
import pandas as pd
import transformers
from transformers import pipeline
nltk.download('stopwords')
nltk.download('punkt')
df_articles = pd.DataFrame()
sentiment_pipeline = pipeline("sentiment-analysis")

from nltk.tokenizer import sent_tokenizer
text = "Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article"



def get_article_text(url):
    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0'

    config = Config()
    config.browser_user_agent = user_agent
    config.request_timeout = 10

    try:



        article = Article(url)
        article.download()
        article.parse()
        article.nlp()
        #df_articles['title'] = article.title
        #df_articles['keywords'] = list(article.keywords)
        #df_articles['summary'] = article.summary

        #df_articles._append(df_articles)
        print(article.title)
        print(article.keywords)
        print(article.summary)


        print(sentiment_pipeline(article.summary))
        df_articles['title_cleaned'] = sentiment_pipeline(df_articles['title'])



        return article.text

    except Exception as e:

        print(f"Error: {e}")



def create_wordcloud(text, keyword):
    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Word Cloud for Articles containing '{keyword}'")
    plt.show()
def main():
    global top_20_words

    keyword = input("Enter the keyword to search: ")
    num_results = int(input("Enter the number of article URLs to process: "))



    full_text = ""
    for url in flattened_list:
        text = get_article_text(url)
        try:


            full_text += text + "\n"


        except Exception as e:


            print(f"Error: {e}")

    # Assuming you already have the 'full_text' variable containing your text
    wordcloud = WordCloud()
    wordcloud.generate_from_text(full_text)
    word_frequencies = wordcloud.process_text(full_text)
    # Convert word_frequencies to a Counter object and get the top 10 most common words
    top_20_words = dict(Counter(word_frequencies).most_common(20))
    print(top_20_words)

    #print(WordCloud().process_text(full_text))
    create_wordcloud(full_text, keyword)
    plt.figure(figsize=(10,10))


if __name__ == "__main__":
    main()

import nltk
import pandas as pd

nltk.download('stopwords')
nltk.download('punkt')


from nltk.tokenizer import sent_tokenizer
text = "Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article"

Keyword = list(top_20_words.keys())
Freq = list(top_20_words.values())
plt.figure(figsize=(10, 5))
plt.xticks(rotation='vertical')

plt.bar(Keyword, Freq)

# Optionally, you can add labels and title
plt.xlabel('Keyword')
plt.ylabel('Freq')
plt.title('Bar Chart from Dictionary')

# Display the chart
plt.show()

!pip install inflect

#import the inflect library
import inflect
p=inflect.engine()

#convert number into words
def convert_number(text):
    #split string into list of words
    temp_str=text.split()
    #initialise empty list
    new_string=[]
    for word in temp_str:
        #if word is a digit,convert the digit
        #to numbers and append into the new_string list
        if word.isdigit():
            temp=p.number_to_words(word)
            new_string.append(temp)
            #append the word as it is
        else:
            new_string.append(word)
    #join the words of new string to form a string
    temp_str=' '.join(new_string)
    return temp_str

input_str='There are 3 balls in this bag, and 12 in the other one.'
print(convert_number(input_str))







"""IGNORE FROM BELOW"""

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service as ChromeService
from bs4 import BeautifulSoup
import time

def is_social_media_or_wikipedia(url):
    return any(domain in url for domain in ['twitter', 'facebook', 'instagram', 'linkedin', 'pinterest', 'wikipedia'])

def get_google_search_links(keyword, num_results):
    url = f"https://www.google.com/search?q={keyword}"

    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)

    try:
        driver.get(url)

        # Scroll down the page to load more results
        for _ in range(num_results // 10):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        # Get the page source containing search results
        page_source = driver.page_source

        # Extract URL links from the search results
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(page_source, 'html.parser')
        links = []
        for result in soup.find_all('div', class_='tF2Cxc'):
            link = result.find('a')
            if link:
                link_url = link['href']
                if not is_social_media_or_wikipedia(link_url):
                    links.append(link_url)

        return links[:num_results]

    except Exception as e:
        print(f"Error: {e}")
        return []

    finally:
        driver.quit()

def main():
    keyword = input("Enter the keyword to search: ")
    num_results = int(input("Enter the number of links to scrape: "))

    article_links = get_google_search_links(keyword, num_results)

    print(f"URL links from Google search results for '{keyword}':")
    for idx, link in enumerate(article_links, 1):
        print(f"{idx}. {link}")

if __name__ == "__main__":
    main()

from urllib.parse import urlparse
from selenium import webdriver
import time
from langdetect import detect
from googletrans import Translator

def get_google_search_links(keyword, num_results=50):
    url = f"https://www.google.com/search?q={keyword}"

    # Set up Selenium webdriver with Chrome
    #driver = webdriver.Chrome(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to chromedriver.exe

    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)



    try:
        driver.get(url)

        # Scroll down the page to load more results
        for _ in range(num_results // 50):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        # Get the page source containing search results
        page_source = driver.page_source

        # Extract URL links from the search results
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(page_source, 'html.parser')
        links = []
        for result in soup.find_all('div', class_='tF2Cxc'):
            link = result.find('a')
            if link:
                links.append(link['href'])

        return links[:num_results]

    except Exception as e:
        print(f"Error: {e}")
        return []

    finally:
        driver.quit()


def translate_page(driver, target_lang='en'):
    translator = Translator()

    # Get the page source
    page_source = driver.page_source

    # Detect the language of the page content
    src_lang = detect(page_source)

    # Translate the page content to the target language
    translated_page = translator.translate(page_source, src=src_lang, dest=target_lang)

    return translated_page.text

#TOP_NEWS_CHANNELS = ['indiatoday', 'thehindu', 'hindustantimes', 'timesofindia', 'ndtv', 'republicworld', 'indianexpress']
SOCIAL_MEDIA_DOMAINS = ['twitter', 'facebook', 'instagram', 'linkedin', 'pinterest','youtube']
WIKIPEDIA_DOMAIN = 'wikipedia'

def is_reputable_news_source(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(channel in domain for channel in TOP_NEWS_CHANNELS)

def is_social_media(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(social in domain for social in SOCIAL_MEDIA_DOMAINS)

def is_wikipedia(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return WIKIPEDIA_DOMAIN in domain

def get_article_details(links, keyword):
    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--lang=en-US,en')

    chrome_options.add_argument('--disable-notifications')
    chrome_options.add_argument('--disable-popup-blocking')
    chrome_options.add_argument('--ignore-certificate-errors')
    chrome_options.add_argument('--disable-infobars')
    chrome_options.add_argument('--disable-web-security')
    chrome_options.add_argument('--allow-running-insecure-content')
    chrome_options.add_argument('--disable-gpu')
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)

    article_details = []
    for link in links:
        try:
            driver.get(link)
            time.sleep(2)  # Add a delay to let the page load completely (you may adjust the delay as needed)

            title = driver.title
            if keyword.lower() in title.lower():  # Check if the keyword is in the article title (case-insensitive)
                if not is_social_media(link) and not is_wikipedia(link):




                    article_details.append((title, link))
                    print(article_details)


                    #article_details.append((title, link))

        except Exception as e:
            print(f"Error while accessing {link}: {e}")

    driver.quit()
    return article_details

def main():
    keywords = input("Enter the keywords to search (comma-separated): ").split(',')
    num_results = int(input("Enter the number of links to scrape: "))

    for keyword in keywords:
        article_links = get_google_search_links(keyword, num_results)
        article_details = get_article_details(article_links, keyword)

        print(f"\nArticle details for '{keyword}' from top news channels:")
        for idx, (title, url) in enumerate(article_details, 1):
            print(f"{idx}. Title: {title}")
            print(f"   URL: {url}")
            print()

if __name__ == "__main__":
    main()



def get_news_article_links(keyword, num_results=10):
    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)

    try:
        search_url = f'https://www.google.com/search?q={keyword}&tbm=nws&tbs=qdr:d'
        driver.get(search_url)

        # Scroll down the page to load more results
        for _ in range(num_results // 10):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        # Get the page source and parse with BeautifulSoup
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')

        # Extract news article links from search results
        links = []
        for result in soup.find_all('div', class_='tF2Cxc'):
            link = result.find('a')
            if link:
                links.append(link['href'])

        return links[:num_results]

    except Exception as e:
        print(f"Error: {e}")
        return []

    finally:
        driver.quit()

def main():
    keyword = input("Enter the keyword to search: ")
    num_results = int(input("Enter the number of links to scrape: "))

    article_links = get_news_article_links(keyword, num_results)

    print(f"News article links related to '{keyword}':")
    for idx, link in enumerate(article_links, 1):
        print(f"{idx}. {link}")

if __name__ == "__main__":
    main()

from selenium import webdriver

def get_google_search_links(keyword, num_results=10):
    url = f"https://www.google.com/search?q={keyword}"

    # Set up Selenium webdriver with Chrome
    #driver = webdriver.Chrome(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to chromedriver.exe

    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)



    try:
        driver.get(url)

        # Scroll down the page to load more results
        for _ in range(num_results // 10):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        # Get the page source containing search results
        page_source = driver.page_source

        # Extract URL links from the search results
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(page_source, 'html.parser')
        links = []
        for result in soup.find_all('div', class_='tF2Cxc'):
            link = result.find('a')
            if link:
                links.append(link['href'])

        return links[:num_results]

    except Exception as e:
        print(f"Error: {e}")
        return []

    finally:
        driver.quit()

def main():
    keyword = input("Enter the keyword to search: ")
    num_results = int(input("Enter the number of links to scrape: "))

    article_links = get_google_search_links(keyword, num_results)

    print(f"URL links from Google search results for '{keyword}':")
    for idx, link in enumerate(article_links, 1):
        print(f"{idx}. {link}")

if __name__ == "__main__":
    main()

from selenium import webdriver
import time

def get_article_details(links):

    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)

    article_details = []
    for link in links:
        try:
            driver.get(link)
            time.sleep(2)  # Add a delay to let the page load completely (you may adjust the delay as needed)

            title = driver.title
            article_details.append((title, driver.current_url))

        except Exception as e:
            print(f"Error while accessing {link}: {e}")

    driver.quit()
    return article_details

def main():
    keyword = input("Enter the keyword to search: ")
    num_results = int(input("Enter the number of links to scrape: "))

    article_links = get_google_search_links(keyword, num_results)
    article_details = get_article_details(article_links)

    print(f"Article details for '{keyword}':")
    for idx, (title, url) in enumerate(article_details, 1):
        print(f"{idx}. Title: {title}")
        print(f"   URL: {url}")
        print()

if __name__ == "__main__":
    main()

from urllib.parse import urlparse
from selenium import webdriver
import time

TOP_NEWS_CHANNELS = ['indiatoday', 'thehindu', 'hindustantimes', 'timesofindia', 'ndtv', 'republicworld', 'indianexpress']

def is_reputable_news_source(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(channel in domain for channel in TOP_NEWS_CHANNELS)

def get_article_details(links):
    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)

    article_details = []
    for link in links:
        try:
            if not is_reputable_news_source(link):
                continue

            driver.get(link)
            time.sleep(2)  # Add a delay to let the page load completely (you may adjust the delay as needed)

            title = driver.title
            article_details.append((title, link))

        except Exception as e:
            print(f"Error while accessing {link}: {e}")

    driver.quit()
    return article_details

def main():
    keyword = input("Enter the keyword to search: ")
    num_results = int(input("Enter the number of links to scrape: "))

    article_links = get_google_search_links(keyword, num_results)
    article_details = get_article_details(article_links)

    print(f"Article details from top news channels for '{keyword}':")
    for idx, (title, url) in enumerate(article_details, 1):
        print(f"{idx}. Title: {title}")
        print(f"   URL: {url}")
        print()

if __name__ == "__main__":
    main()

from urllib.parse import urlparse
from selenium import webdriver
import time

TOP_NEWS_CHANNELS = ['indiatoday', 'thehindu', 'hindustantimes', 'timesofindia', 'ndtv', 'republicworld', 'indianexpress']

def is_reputable_news_source(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(channel in domain for channel in TOP_NEWS_CHANNELS)

def get_article_details(links, keyword):
    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)

    article_details = []
    for link in links:
        try:
            driver.get(link)
            time.sleep(2)  # Add a delay to let the page load completely (you may adjust the delay as needed)

            title = driver.title
            if keyword.lower() in title.lower():  # Check if the keyword is in the article title (case-insensitive)
                if is_reputable_news_source(link):  # Check if the URL belongs to a reputable news source
                    article_details.append((title, link))

        except Exception as e:
            print(f"Error while accessing {link}: {e}")

    driver.quit()
    return article_details

def main():
    keyword = input("Enter the keyword to search: ")
    num_results = int(input("Enter the number of links to scrape: "))

    article_links = get_google_search_links(keyword, num_results)
    article_details = get_article_details(article_links, keyword)

    print(f"Article details from top news channels containing '{keyword}':")
    for idx, (title, url) in enumerate(article_details, 1):
        print(f"{idx}. Title: {title}")
        print(f"   URL: {url}")
        print()

if __name__ == "__main__":
    main()

from urllib.parse import urlparse
from selenium import webdriver
import time
from langdetect import detect
from googletrans import Translator

def translate_page(driver, target_lang='en'):
    translator = Translator()

    # Get the page source
    page_source = driver.page_source

    # Detect the language of the page content
    src_lang = detect(page_source)

    # Translate the page content to the target language
    translated_page = translator.translate(page_source, src=src_lang, dest=target_lang)

    return translated_page.text

#TOP_NEWS_CHANNELS = ['indiatoday', 'thehindu', 'hindustantimes', 'timesofindia', 'ndtv', 'republicworld', 'indianexpress']
SOCIAL_MEDIA_DOMAINS = ['twitter', 'facebook', 'instagram', 'linkedin', 'pinterest','youtube']
WIKIPEDIA_DOMAIN = 'wikipedia'

def is_reputable_news_source(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(channel in domain for channel in TOP_NEWS_CHANNELS)

def is_social_media(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(social in domain for social in SOCIAL_MEDIA_DOMAINS)

def is_wikipedia(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return WIKIPEDIA_DOMAIN in domain

def get_article_details(links, keyword):
    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument('--lang=en-US,en')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36')
    chrome_options.add_argument('--disable-notifications')
    chrome_options.add_argument('--disable-popup-blocking')
    chrome_options.add_argument('--ignore-certificate-errors')
    chrome_options.add_argument('--disable-infobars')
    chrome_options.add_argument('--disable-web-security')
    chrome_options.add_argument('--allow-running-insecure-content')
    chrome_options.add_argument('--disable-gpu')
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)

    article_details = []
    for link in links:
        try:
            driver.get(link)
            time.sleep(2)  # Add a delay to let the page load completely (you may adjust the delay as needed)

            title = driver.title
            if keyword.lower() in title.lower():  # Check if the keyword is in the article title (case-insensitive)
                if not is_social_media(link) and not is_wikipedia(link):




                    article_details.append((title, link))


                    #article_details.append((title, link))

        except Exception as e:
            print(f"Error while accessing {link}: {e}")

    driver.quit()
    return article_details

def main():
    keywords = input("Enter the keywords to search (comma-separated): ").split(',')
    num_results = int(input("Enter the number of links to scrape: "))

    for keyword in keywords:
        article_links = get_google_search_links(keyword, num_results)
        article_details = get_article_details(article_links, keyword)

        print(f"\nArticle details for '{keyword}' from top news channels:")
        for idx, (title, url) in enumerate(article_details, 1):
            print(f"{idx}. Title: {title}")
            print(f"   URL: {url}")
            print()

if __name__ == "__main__":
    main()

article_details

!pip install googletrans==4.0.0-rc1

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

TOP_NEWS_CHANNELS = ['indiatoday', 'thehindu', 'hindustantimes', 'timesofindia', 'ndtv', 'republicworld', 'indianexpress']
SOCIAL_MEDIA_DOMAINS = ['twitter', 'facebook', 'instagram', 'linkedin', 'pinterest']
WIKIPEDIA_DOMAIN = 'wikipedia'

def is_reputable_news_source(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(channel in domain for channel in TOP_NEWS_CHANNELS)

def is_social_media(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(social in domain for social in SOCIAL_MEDIA_DOMAINS)

def is_wikipedia(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return WIKIPEDIA_DOMAIN in domain

def handle_popups(driver):
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'popup-class')))
        driver.find_element_by_class_name('popup-class').click()
    except Exception as e:
        print("Popup not found or failed to close:", e)

def get_article_details(links, keyword):
    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)

    article_details = []
    for link in links:
        try:
            driver.get(link)
            time.sleep(2)  # Add a delay to let the page load completely (you may adjust the delay as needed)

            handle_popups(driver)  # Handle popups if they appear

            title = driver.title
            if keyword.lower() in title.lower():  # Check if the keyword is in the article title (case-insensitive)
                if is_reputable_news_source(link) and not is_social_media(link) and not is_wikipedia(link):
                    article_details.append((title, link))

        except Exception as e:
            print(f"Error while accessing {link}: {e}")

    driver.quit()
    return article_details

def main():
    keyword = input("Enter the keyword to search: ")
    num_results = int(input("Enter the number of links to scrape: "))

    article_links = get_article_details(keyword, num_results)
    article_details = get_article_details(article_links, keyword)

    print(f"Article details from top news channels containing '{keyword}':")
    for idx, (title, url) in enumerate(article_details, 1):
        print(f"{idx}. Title: {title}")
        print(f"   URL: {url}")
        print()

if __name__ == "__main__":
    main()

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
from urllib.parse import urlparse

TOP_NEWS_CHANNELS = ['indiatoday', 'thehindu', 'hindustantimes', 'timesofindia', 'ndtv', 'republicworld', 'indianexpress']
SOCIAL_MEDIA_DOMAINS = ['twitter', 'facebook', 'instagram', 'linkedin', 'pinterest']
WIKIPEDIA_DOMAIN = 'wikipedia'

def get_google_search_links(keyword, num_results):
    url = f"https://www.google.com/search?q={keyword}"

    # Set up Selenium webdriver with Chrome
    #driver = webdriver.Chrome(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to chromedriver.exe

    chrome_service = ChromeService(executable_path=r'C:\Users\User\Downloads\chromedriver_win32\chromedriver.exe')  # Replace with the path to your chromedriver executable
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(service=chrome_service, options=chrome_options)



    try:
        driver.get(url)

        # Scroll down the page to load more results
        for _ in range(num_results // 10):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        # Get the page source containing search results
        page_source = driver.page_source

        # Extract URL links from the search results
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(page_source, 'html.parser')
        links = []
        for result in soup.find_all('div', class_='tF2Cxc'):
            link = result.find('a')
            if link:
                links.append(link['href'])

        return links[:num_results]

    except Exception as e:
        print(f"Error: {e}")
        return []

    finally:
        driver.quit()



def is_reputable_news_source(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(channel in domain for channel in TOP_NEWS_CHANNELS)

def is_social_media(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return any(social in domain for social in SOCIAL_MEDIA_DOMAINS)

def is_wikipedia(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    return WIKIPEDIA_DOMAIN in domain

def handle_popups(driver):
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'popup-class')))
        driver.find_element_by_class_name('popup-class').click()
    except Exception as e:
        print("Popup not found or failed to close:", e)

def get_article_details(links, keyword):
    chrome_options = webdriver.ChromeOptions()
    #chrome_options.add_argument('--headless')  # Run Chrome in headless mode to hide the browser window
    driver = webdriver.Chrome(options=chrome_options)

    article_details = []
    for link in links:
        try:
            driver.get(link)
            time.sleep(2)  # Add a delay to let the page load completely (you may adjust the delay as needed)

            #handle_popups(driver)  # Handle popups if they appear

            title = driver.title
            if keyword.lower() in title.lower():  # Check if the keyword is in the article title (case-insensitive)
                if not is_social_media(link) and not is_wikipedia(link):
                    article_details.append((title, link))

            continue
        except Exception as e:
            print(f"Error while accessing {link}: {e}")

    driver.quit()
    return article_details

def main():
    keyword = input("Enter the keyword to search: ")
    num_results = int(input("Enter the number of links to scrape: "))
    links = get_google_search_links(keyword,num_results)

    article_details = get_article_details(links, keyword)

    print(f"Article details from top news channels containing '{keyword}':")
    for idx, (title, url) in enumerate(article_details, 1):
        print(f"{idx}. Title: {title}")
        print(f"   URL: {url}")
        print()

if __name__ == "__main__":
    main()

search_url = f'https://www.google.com/search?q=new&tbm=nws&tbs=qdr:d'
driver.get(search_url)

article_links = get_news_article_links('Modi', 10)

article_links